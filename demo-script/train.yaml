# ============================================================================
# Travel Planner - 多步骤智能体配置文件
# ============================================================================
# 本配置文件用于训练具有多轮交互能力的旅行规划智能体
# 使用AgentScope框架 + ReAct工作流 + 搜索工具
# 算法: Multi-Step GRPO (支持多步骤强化学习)
# ============================================================================

# ----------------------------------------------------------------------------
# 项目基本信息
# ----------------------------------------------------------------------------
project: travelPlanner              # 项目名称
name: travelPlanner                 # 实验名称
checkpoint_root_dir: /home/ecs-user/travelPlanner/checkpoints
                                    # checkpoint保存目录

# ----------------------------------------------------------------------------
# 算法配置 - Multi-Step GRPO
# ----------------------------------------------------------------------------
algorithm:
  algorithm_type: multi_step_grpo   # 多步骤GRPO算法
                                    # 与普通GRPO区别: 支持多轮交互，每一步都有独立的经验
                                    # 适用场景: ReAct、对话代理、游戏AI等多步任务
  
  repeat_times: 1                   # 每个任务重复8次
                                    # 用于GRPO算法的group归一化
  
  optimizer:
    lr: 1e-6                        # 学习率（较小，适合多步骤任务）

# ----------------------------------------------------------------------------
# 模型配置
# ----------------------------------------------------------------------------
model:
  model_path: /home/ecs-user/models/Qwen2-7B-Instruct
                                    # 主模型路径
  
  max_response_tokens: 4096         # 每步最大生成tokens
                                    # 多步任务中每步的回复长度
  
  max_model_len: 20480              # 模型支持的最大序列长度（20K tokens）
                                    # 需要较长上下文以容纳多轮对话历史
# ----------------------------------------------------------------------------
# 集群配置
# ----------------------------------------------------------------------------
cluster:
  node_num: 1                       # 使用1个节点
  gpu_per_node: 8                   # 每个节点8块GPU

# ----------------------------------------------------------------------------
# 数据缓冲区配置
# ----------------------------------------------------------------------------
buffer:
  total_epochs: 8                   # 训练8轮（多步骤任务通常需要更多轮次）
  
  batch_size: 16                    # 每批16个任务
                                    # 多步骤任务的batch size通常较小
                                    # 因为每个任务会产生多个step的经验
  
  train_batch_size: 640             # 训练批次: 16*8*4 = 512
                                    # = batch_size * repeat_times * 平均步数
  # Explorer输入配置
  explorer_input:
    # 训练任务集
    taskset:
      name: webqa_train             # 训练集名称
      
      storage_type: file            # 文件存储类型
      
      path: '/home/ecs-user/datasets'  # 数据集路径
                                     
      
      split: train                  # 使用训练集
      
      # 数据格式配置
      format:
        prompt_key: 'query'       # 问题字段名
        response_key: 'annotated_plan'      # 答案字段名


      
      # Workflow参数（传递给workflow的配置）
      workflow_args:
        max_iters: 1               # 最大交互轮数
        similarity_model_name: "all-MiniLM-L6-v2"
        total_tools: 6
                                     
      # 采样参数
      rollout_args:
        temperature: 1.0            # 训练时使用随机采样
        max_tokens: 4096            # 每步最大生成tokens
      
      enable_progress_bar: false    # 不显示进度条（避免日志混乱）
      
      default_workflow_type: 'travel_planner_workflow'
                                  
    # 评估任务集
    eval_tasksets:
    - name: webqa_test              # 测试集名称
      storage_type: file
      path: '/home/ecs-user/datasets'  # 同一数据集路径
      split: test                   # 使用测试集
      
      format:
        prompt_key: 'query'
        response_key: 'annotated_plan'
      
      enable_progress_bar: false
      
      workflow_args:
        max_iters: 1               # 评估时也允许最多10步
        similarity_model_name: "all-MiniLM-L6-v2"
        total_tools: 6
      
      rollout_args:
        temperature: 0.6            # 评估时降低随机性（更稳定的输出）
                                    # 注意: 不是0.0，保留一定随机性避免重复
        max_tokens: 4096
      
      default_workflow_type: 'travel_planner_workflow'
  # Trainer输入配置
  trainer_input:
    experience_buffer:
      name: experience_buffer       # 经验缓冲区名称
      
      storage_type: queue           # 队列类型（在线学习）
      
      # 经验回放配置（重要：提高样本利用率）
      replay_buffer:
        enable: true                # 启用经验回放
                                    # 多步骤任务强烈建议启用！
                                    # 因为采样成本高，需要充分利用已有经验
# ----------------------------------------------------------------------------
# Explorer配置 - 多步骤智能体的推理配置
# ----------------------------------------------------------------------------
explorer:
  eval_interval: 10                 # 每10步评估一次
                                    # 多步骤任务评估较慢，间隔可以设大一些
  
  max_repeat_times_per_runner: 1   # 每个runner处理任务的重复次数
                                    # 1表示任务不在runner级别重复
                                    # 重复在算法层面通过repeat_times控制
  
  max_timeout: 3600                 # 单个任务最大执行时间: 60分钟
                                    # 多步骤任务可能需要很长时间
                                    # 包括多次模型调用 + 工具调用
  
  # 推理模型配置（主模型）
  rollout_model:
    # 功能开关（多步骤任务的关键配置）
    enable_thinking: true           # 启用thinking模式（Qwen3特性）
                                    # 模型会在<think>标签中进行推理
    
    enable_history: true            # ⚠️ 必须启用！
                                    # 多步骤任务需要记录对话历史
                                    # 每一步的输出会添加到历史中
    
    enable_openai_api: true         # 启用OpenAI兼容API
                                    # AgentScope workflow使用OpenAI接口
    
    enable_auto_tool_choice: true   # 自动选择工具
                                    # 模型会根据任务自动决定调用哪个工具
    
    tool_call_parser: hermes        # 工具调用解析器
                                    # hermes: Hermes格式的工具调用解析
                                    # 其他选项: qwen, functionary等
    
    # 引擎配置
    engine_num: 2                   # 4个推理引擎（并行处理多个agent）
    
    tensor_parallel_size: 1         # 单GPU推理
    
    enable_prefix_caching: false    # 不启用前缀缓存
                                    # 多步任务每步的prompt都不同，缓存效果有限
    
    enforce_eager: false            # 使用CUDA图加速（提高速度）
    
    dtype: bfloat16                 # 半精度推理
    
    seed: 42                        # 随机种子
    
    gpu_memory_utilization: 0.8     # 使用80%显存
                                    # 多步任务可能需要更多显存存储历史
    
    enable_chunked_prefill: true    # 启用分块预填充
                                    # 有助于处理多步任务产生的长上下文
  # 辅助模型配置（可选）
  auxiliary_models:                 #  用于语义评估奖励函数的 model
    # 辅助模型1: 用于特定子任务
    - model_path: /home/ecs-user/models/Qwen2.5-0.5B-Instruct
                                    # 辅助模型路径（通常是较小的模型）
                                    # 用途示例: 
                                    # - Reward model（评估agent行为）
                                    # - Critic model（价值估计）
                                    # - 特定任务的专家模型
      
      engine_num: 1                 # 1个推理引擎
      
      tensor_parallel_size: 1        
                                     
      
      enable_thinking: false        # 辅助模型不需要thinking
      
      # 长度配置（辅助模型通常处理更长的上下文）
      max_prompt_tokens: 20480      # 最大输入20K tokens
      max_response_tokens: 512    # 最大输出10K tokens
      max_model_len: 32000          # 总长度32K tokens
# ----------------------------------------------------------------------------
# 同步器配置 - Explorer和Trainer的权重同步
# ----------------------------------------------------------------------------
synchronizer:
  sync_style: dynamic_by_explorer   # 动态同步（由Explorer驱动）
                                    # Explorer完成一定数量的任务后触发同步
                                    # 适合多步骤任务（采样时间长）
                                    # 其他选项:
                                    # - fixed: 固定间隔同步
                                    # - dynamic_by_trainer: Trainer驱动同步
  
  sync_method: 'nccl'               # 使用NCCL通信（GPU间高速通信）
  
  sync_interval: 5                  # 每5步同步一次
                                    # 多步骤任务可以设置较大的间隔
                                    # 因为每步采样成本高
  
  sync_timeout: 3600                # 同步超时: 60分钟

# ----------------------------------------------------------------------------
# Trainer配置 - 模型训练参数
# ----------------------------------------------------------------------------
trainer:
  save_interval: 20                 # 每20步保存checkpoint
                                    # 多步骤任务保存间隔可以大一些
  
  grad_clip: 1.0                    # 梯度裁剪（防止梯度爆炸）
  
  use_dynamic_bsz: true             # 动态批次大小
                                    # 根据序列长度自动调整
  
  max_token_len_per_gpu: 16384      # 每个GPU最大16K tokens
                                    # 多步骤任务需要处理长序列
  
  ulysses_sequence_parallel_size: 2 # 2路序列并行
                                    # 将长序列分片到2个GPU处理

